{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb2b346-104f-4c59-9f24-ef955d00d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "# Basic OS, date/time, and numerical libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for data handling and scientific computing\n",
    "import xarray as xr\n",
    "import dask.array as da # For handling large arrays that don't fit in memory\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # For functions like F.pad\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch Lightning for streamlining training\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas for data manipulation (e.g., creating submission CSV)\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e1f3a3-b5e7-4d49-a588-1e1b836b99d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "# Main configuration dictionary for the experiment\n",
    "\n",
    "# NOTE: You MUST change the 'path' in config['data'] to the correct location \n",
    "# of your 'processed_data_cse151b_v2_corrupted_ssp245.zarr' file.\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"], # Input climate forcing variables\n",
    "        \"output_vars\": [\"tas\", \"pr\"], # Target variables: surface air temperature and precipitation\n",
    "        \"target_member_id\": 0, # Ensemble member to use for target variables\n",
    "        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"], # SSP scenarios for training\n",
    "        \"test_ssp\": \"ssp245\",  # SSP scenario for testing (held-out)\n",
    "        \"test_months\": 360,   # Number of months for the test split (last 10 years)\n",
    "        \"batch_size\": 64,     # Batch size for training and evaluation\n",
    "        \"num_workers\": 4,     # Number of workers for DataLoader\n",
    "    },\n",
    "    \"model_unet\": { # Configuration specific to U-Net\n",
    "        \"type\": \"unet\",\n",
    "        \"init_features\": 64, # Number of features in the first convolutional layer of U-Net\n",
    "        \"bilinear\": True,    # Whether to use bilinear upsampling in U-Net's decoder\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"lr\": 1e-3, # Learning rate\n",
    "        # Add other training parameters like weight_decay if needed\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 20,         # Maximum number of training epochs\n",
    "        \"accelerator\": \"auto\",    # Auto-detect accelerator (CPU, GPU, TPU)\n",
    "        \"devices\": \"auto\",        # Auto-detect number of devices\n",
    "        \"precision\": 32,          # Training precision (e.g., 16 for mixed-precision)\n",
    "        \"deterministic\": True,    # For reproducibility\n",
    "        \"num_sanity_val_steps\": 0,# Number of sanity check validation steps before training\n",
    "        # \"logger\": True, # Example: WandbLogger(...) or TensorBoardLogger(...)\n",
    "        # \"callbacks\": [] # Example: [ModelCheckpoint(...), EarlyStopping(...)]\n",
    "    },\n",
    "    \"seed\": 42, # Seed for reproducibility\n",
    "}\n",
    "\n",
    "# Set seed for PyTorch Lightning, PyTorch, NumPy, and Python's random module\n",
    "pl.seed_everything(config[\"seed\"], workers=True) \n",
    "\n",
    "# Suggestion from PyTorch for Tensor Core utilization on compatible GPUs\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7: # Check for Volta architecture or newer\n",
    "    torch.set_float32_matmul_precision('medium') # or 'high'\n",
    "    print(\"Set torch.set_float32_matmul_precision('medium') for Tensor Core utilization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8df60e-21e9-4e7b-a414-e1b5afde2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Latitude Weights Utility\n",
    "\n",
    "def get_lat_weights(latitude_values):\n",
    "    \"\"\"\n",
    "    Computes cosine-based area weights for each latitude.\n",
    "    This accounts for the Earth's curvature, giving more weight to\n",
    "    grid cells near the equator for global metrics.\n",
    "\n",
    "    Args:\n",
    "        latitude_values (np.array): Array of latitude values in degrees.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Normalized latitude weights.\n",
    "    \"\"\"\n",
    "    lat_rad = np.deg2rad(latitude_values) # Convert degrees to radians\n",
    "    weights = np.cos(lat_rad)             # Cosine of latitude\n",
    "    return weights / np.mean(weights)     # Normalize by the mean weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032abb3c-54cd-417d-93f8-9fe55d3a4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Normalizer Class\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"\n",
    "    Handles Z-score normalization for input and output data.\n",
    "    (data - mean) / std\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_in, self.std_in = None, None   # Statistics for input data\n",
    "        self.mean_out, self.std_out = None, None # Statistics for output data\n",
    "\n",
    "    def set_input_statistics(self, mean, std):\n",
    "        \"\"\"Sets the mean and standard deviation for input data.\"\"\"\n",
    "        self.mean_in = mean\n",
    "        self.std_in = std\n",
    "\n",
    "    def set_output_statistics(self, mean, std):\n",
    "        \"\"\"Sets the mean and standard deviation for output data.\"\"\"\n",
    "        self.mean_out = mean\n",
    "        self.std_out = std\n",
    "\n",
    "    def normalize(self, data, data_type):\n",
    "        \"\"\"\n",
    "        Normalizes the data using pre-computed statistics.\n",
    "\n",
    "        Args:\n",
    "            data (np.array or dask.array): Data to normalize.\n",
    "            data_type (str): \"input\" or \"output\", to use appropriate statistics.\n",
    "\n",
    "        Returns:\n",
    "            Normalized data.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If statistics for the specified data_type are not set.\n",
    "        \"\"\"\n",
    "        if data_type == \"input\":\n",
    "            if self.mean_in is None or self.std_in is None:\n",
    "                raise ValueError(\"Input statistics not set in Normalizer.\")\n",
    "            # Add a small epsilon to std to prevent division by zero if std is very small or zero\n",
    "            return (data - self.mean_in) / (self.std_in + 1e-8) \n",
    "        elif data_type == \"output\":\n",
    "            if self.mean_out is None or self.std_out is None:\n",
    "                raise ValueError(\"Output statistics not set in Normalizer.\")\n",
    "            return (data - self.mean_out) / (self.std_out + 1e-8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data_type '{data_type}'. Must be 'input' or 'output'.\")\n",
    "\n",
    "    def inverse_transform_output(self, data):\n",
    "        \"\"\"\n",
    "        Applies inverse normalization to output data (predictions).\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor or np.array): Normalized output data.\n",
    "\n",
    "        Returns:\n",
    "            Data in original physical units.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If output statistics are not set.\n",
    "        \"\"\"\n",
    "        if self.mean_out is None or self.std_out is None:\n",
    "            raise ValueError(\"Output statistics not set in Normalizer for inverse transform.\")\n",
    "        return data * (self.std_out + 1e-8) + self.mean_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8e1fce-c1b0-43cb-9204-de384f73f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: U-Net Model Architecture\n",
    "# (DoubleConv, Down, Up, OutConv, UNet classes)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels_x1, in_channels_x2, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # in_channels_x1: channels of the feature map from the upsampling path (lower layer in decoder)\n",
    "        # in_channels_x2: channels of the feature map from the skip connection (encoder)\n",
    "        # out_channels: channels produced by this Up block\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            # After upsampling x1, its channel count (in_channels_x1) remains the same.\n",
    "            # It's then concatenated with x2 (in_channels_x2).\n",
    "            # So, the input to DoubleConv is (in_channels_x1 + in_channels_x2).\n",
    "            self.conv = DoubleConv(in_channels_x1 + in_channels_x2, out_channels)\n",
    "        else:\n",
    "            # ConvTranspose2d halves the channels of x1 (in_channels_x1 -> in_channels_x1 // 2)\n",
    "            self.up = nn.ConvTranspose2d(in_channels_x1, in_channels_x1 // 2, kernel_size=2, stride=2)\n",
    "            # Input to DoubleConv is (in_channels_x1 // 2 + in_channels_x2)\n",
    "            self.conv = DoubleConv(in_channels_x1 // 2 + in_channels_x2, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: feature map from upsampling path (e.g., from x_bottleneck or previous Up layer)\n",
    "        # x2: feature map from skip connection (encoder path, e.g., x4_skip)\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Pad x1 to match x2's spatial dimensions if they differ\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1) # Concatenate along channel dimension\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_input_channels, n_output_channels, bilinear=True, init_features=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.bilinear = bilinear\n",
    "        self.init_features = init_features \n",
    "        \n",
    "        f = init_features # Short alias\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_input_channels, f)\n",
    "        self.down1 = Down(f, f * 2)\n",
    "        self.down2 = Down(f * 2, f * 4)\n",
    "        self.down3 = Down(f * 4, f * 8)\n",
    "        self.down4 = Down(f * 8, f * 16) # Bottleneck input features\n",
    "\n",
    "        # Decoder\n",
    "        # Arguments for Up: (channels_from_lower_upsampled_layer, channels_from_skip_connection, output_channels_for_this_Up_block)\n",
    "        self.up1 = Up(f * 16, f * 8,  f * 8, bilinear)\n",
    "        self.up2 = Up(f * 8,  f * 4,  f * 4, bilinear)\n",
    "        self.up3 = Up(f * 4,  f * 2,  f * 2, bilinear)\n",
    "        self.up4 = Up(f * 2,  f,      f,     bilinear)\n",
    "        \n",
    "        self.outc = OutConv(f, n_output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_skip = self.inc(x)           # -> f\n",
    "        x2_skip = self.down1(x1_skip)   # -> f * 2\n",
    "        x3_skip = self.down2(x2_skip)   # -> f * 4\n",
    "        x4_skip = self.down3(x3_skip)   # -> f * 8\n",
    "        x_bottleneck = self.down4(x4_skip) # -> f * 16\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x_bottleneck, x4_skip) # x_bottleneck (f*16), x4_skip (f*8). Up outputs f*8\n",
    "        x = self.up2(x, x3_skip)            # x (f*8), x3_skip (f*4). Up outputs f*4\n",
    "        x = self.up3(x, x2_skip)            # x (f*4), x2_skip (f*2). Up outputs f*2\n",
    "        x = self.up4(x, x1_skip)            # x (f*2), x1_skip (f). Up outputs f\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7d3515-bfac-483c-af65-198f6dc952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ClimateDataset and ClimateDataModule\n",
    "\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, inputs_dask, outputs_dask, output_is_normalized=True):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset for climate data.\n",
    "\n",
    "        Args:\n",
    "            inputs_dask (dask.array): Dask array of input features.\n",
    "            outputs_dask (dask.array): Dask array of output targets.\n",
    "            output_is_normalized (bool): Flag indicating if outputs_dask is already normalized.\n",
    "                                         Used for the test set where targets are not pre-normalized.\n",
    "        \"\"\"\n",
    "        self.size = inputs_dask.shape[0]\n",
    "        print(f\"Creating dataset with {self.size} samples...\")\n",
    "\n",
    "        inputs_np = inputs_dask.compute()\n",
    "        outputs_np = outputs_dask.compute()\n",
    "\n",
    "        self.inputs = torch.from_numpy(inputs_np).float()\n",
    "        self.outputs = torch.from_numpy(outputs_np).float()\n",
    "\n",
    "        if torch.isnan(self.inputs).any():\n",
    "            raise ValueError(\"NaNs found in input dataset after converting to tensor.\")\n",
    "        if torch.isnan(self.outputs).any():\n",
    "            raise ValueError(\"NaNs found in output dataset after converting to tensor.\")\n",
    "        \n",
    "        print(f\"Dataset created. Input shape: {self.inputs.shape}, Output shape: {self.outputs.shape}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "class ClimateDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        input_vars,\n",
    "        output_vars,\n",
    "        train_ssps,\n",
    "        test_ssp,\n",
    "        target_member_id,\n",
    "        test_months=120,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        seed=42, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not os.path.exists(self.hparams.path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {self.hparams.path}. Please check config['data']['path'].\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds = xr.open_zarr(self.hparams.path, consolidated=False, chunks={\"time\": 24})\n",
    "        \n",
    "        # --- FIX for spatial_template ---\n",
    "        # The 'rsdt' variable might not have 'member_id'. Handle this conditionally.\n",
    "        rsdt_var_for_template = ds[\"rsdt\"]\n",
    "        if \"member_id\" in rsdt_var_for_template.dims:\n",
    "            spatial_template = rsdt_var_for_template.isel(time=0, ssp=0, member_id=0, drop=True)\n",
    "        else:\n",
    "            # If 'member_id' is not present, select without it.\n",
    "            # This assumes 'rsdt' is consistent across members or doesn't have that dimension.\n",
    "            spatial_template = rsdt_var_for_template.isel(time=0, ssp=0, drop=True)\n",
    "        # --- END FIX ---\n",
    "\n",
    "        def load_ssp(ssp_name):\n",
    "            input_dask_list, output_dask_list = [], []\n",
    "            \n",
    "            for var_name in self.hparams.input_vars:\n",
    "                da_var = ds[var_name].sel(ssp=ssp_name)\n",
    "                if \"latitude\" in da_var.dims: \n",
    "                    da_var = da_var.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                # For input variables, if member_id exists, select the target_member_id.\n",
    "                # If it doesn't exist (e.g. for some forcing data), this sel will be a no-op if strict=False,\n",
    "                # or we can check existence. Xarray's sel is usually robust if the dim doesn't exist.\n",
    "                # However, to be safe, let's check.\n",
    "                if \"member_id\" in da_var.dims:\n",
    "                    da_var = da_var.sel(member_id=self.hparams.target_member_id)\n",
    "                \n",
    "                if set(da_var.dims) == {\"time\"}: \n",
    "                    da_var = da_var.broadcast_like(spatial_template).transpose(\"time\", \"y\", \"x\")\n",
    "                input_dask_list.append(da_var.data)\n",
    "            \n",
    "            for var_name in self.hparams.output_vars:\n",
    "                # Output variables are always selected by target_member_id\n",
    "                da_out = ds[var_name].sel(ssp=ssp_name, member_id=self.hparams.target_member_id)\n",
    "                if \"latitude\" in da_out.dims: \n",
    "                    da_out = da_out.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                output_dask_list.append(da_out.data)\n",
    "\n",
    "            return da.stack(input_dask_list, axis=1), da.stack(output_dask_list, axis=1)\n",
    "\n",
    "        train_input_list, train_output_list = [], []\n",
    "        val_input_ssp370, val_output_ssp370 = None, None\n",
    "\n",
    "        for ssp in self.hparams.train_ssps:\n",
    "            x_ssp, y_ssp = load_ssp(ssp)\n",
    "            if ssp == \"ssp370\": \n",
    "                val_input_ssp370 = x_ssp[-self.hparams.test_months:]\n",
    "                val_output_ssp370 = y_ssp[-self.hparams.test_months:]\n",
    "                train_input_list.append(x_ssp[:-self.hparams.test_months])\n",
    "                train_output_list.append(y_ssp[:-self.hparams.test_months])\n",
    "            else:\n",
    "                train_input_list.append(x_ssp)\n",
    "                train_output_list.append(y_ssp)\n",
    "        \n",
    "        train_input_all_ssp = da.concatenate(train_input_list, axis=0)\n",
    "        train_output_all_ssp = da.concatenate(train_output_list, axis=0)\n",
    "\n",
    "        input_mean = da.nanmean(train_input_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        input_std = da.nanstd(train_input_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        self.normalizer.set_input_statistics(mean=input_mean, std=input_std)\n",
    "\n",
    "        output_mean = da.nanmean(train_output_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        output_std = da.nanstd(train_output_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        self.normalizer.set_output_statistics(mean=output_mean, std=output_std)\n",
    "\n",
    "        train_input_norm = self.normalizer.normalize(train_input_all_ssp, \"input\")\n",
    "        train_output_norm = self.normalizer.normalize(train_output_all_ssp, \"output\")\n",
    "        \n",
    "        val_input_norm = self.normalizer.normalize(val_input_ssp370, \"input\")\n",
    "        val_output_norm = self.normalizer.normalize(val_output_ssp370, \"output\")\n",
    "\n",
    "        test_input_ssp, test_output_ssp = load_ssp(self.hparams.test_ssp)\n",
    "        test_input_ssp = test_input_ssp[-self.hparams.test_months:] \n",
    "        test_output_ssp = test_output_ssp[-self.hparams.test_months:]\n",
    "        test_input_norm = self.normalizer.normalize(test_input_ssp, \"input\")\n",
    "\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = ClimateDataset(train_input_norm, train_output_norm)\n",
    "            self.val_dataset = ClimateDataset(val_input_norm, val_output_norm)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = ClimateDataset(test_input_norm, test_output_ssp, output_is_normalized=False)\n",
    "        \n",
    "        self.lat = spatial_template.y.values\n",
    "        self.lon = spatial_template.x.values\n",
    "        self.area_weights = xr.DataArray(get_lat_weights(self.lat), dims=[\"y\"], coords={\"y\": self.lat})\n",
    "\n",
    "        ds.close()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def get_lat_weights(self):\n",
    "        return self.area_weights\n",
    "\n",
    "    def get_coords(self):\n",
    "        return self.lat, self.lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e752dae9-03d3-4295-9c6f-408b1e2e4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ClimateEmulationModule (PyTorch Lightning)\n",
    "\n",
    "class ClimateEmulationModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.save_hyperparameters(ignore=['model']) \n",
    "        \n",
    "        self.criterion = nn.MSELoss() \n",
    "        self.normalizer = None \n",
    "        \n",
    "        self.val_preds, self.val_targets = [], []\n",
    "        self.test_preds, self.test_targets = [], []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _get_normalizer_from_datamodule(self):\n",
    "        \"\"\"Helper to safely get normalizer from datamodule.\"\"\"\n",
    "        if hasattr(self.trainer, 'datamodule') and self.trainer.datamodule is not None and hasattr(self.trainer.datamodule, 'normalizer'):\n",
    "            return self.trainer.datamodule.normalizer\n",
    "        else:\n",
    "            # Fallback if trainer.datamodule is not set up (e.g. direct call to test without fit)\n",
    "            # This requires 'config' to be globally accessible or passed differently.\n",
    "            print(\"Warning: Normalizer not found via self.trainer.datamodule. Attempting fallback initialization.\")\n",
    "            temp_dm = ClimateDataModule(**config[\"data\"]) \n",
    "            temp_dm.prepare_data()\n",
    "            temp_dm.setup(stage=\"test\") # Or appropriate stage to ensure normalizer stats are computed\n",
    "            return temp_dm.normalizer\n",
    "\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Called at the beginning of training.\"\"\"\n",
    "        self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "    def on_test_start(self):\n",
    "        \"\"\"Called at the beginning of testing.\"\"\"\n",
    "        if self.normalizer is None: # Ensure normalizer is available\n",
    "            self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_norm = batch \n",
    "        y_hat_norm = self(x)   \n",
    "        loss = self.criterion(y_hat_norm, y_norm)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_norm = batch\n",
    "        y_hat_norm = self(x)\n",
    "        loss = self.criterion(y_hat_norm, y_norm)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "        y_hat_denorm = self.normalizer.inverse_transform_output(y_hat_norm.detach().cpu().numpy())\n",
    "        y_denorm = self.normalizer.inverse_transform_output(y_norm.detach().cpu().numpy())\n",
    "        \n",
    "        self.val_preds.append(y_hat_denorm)\n",
    "        self.val_targets.append(y_denorm)\n",
    "        return loss \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking: # Skip during sanity check\n",
    "            if not self.val_preds or not self.val_targets: \n",
    "                return\n",
    "\n",
    "            preds_epoch = np.concatenate(self.val_preds, axis=0)\n",
    "            trues_epoch = np.concatenate(self.val_targets, axis=0)\n",
    "            \n",
    "            if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "            \n",
    "            self._evaluate(preds_epoch, trues_epoch, phase=\"val\")\n",
    "            \n",
    "            np.save(\"val_preds.npy\", preds_epoch)\n",
    "            np.save(\"val_trues.npy\", trues_epoch)\n",
    "            \n",
    "            self.val_preds.clear() \n",
    "            self.val_targets.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y_true_denorm = batch \n",
    "        y_hat_norm = self(x)    \n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "        \n",
    "        y_hat_denorm = self.normalizer.inverse_transform_output(y_hat_norm.detach().cpu().numpy())\n",
    "        \n",
    "        self.test_preds.append(y_hat_denorm)\n",
    "        self.test_targets.append(y_true_denorm.detach().cpu().numpy()) \n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if not self.test_preds or not self.test_targets: \n",
    "            return\n",
    "\n",
    "        preds_epoch = np.concatenate(self.test_preds, axis=0)\n",
    "        trues_epoch = np.concatenate(self.test_targets, axis=0)\n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "        self._evaluate(preds_epoch, trues_epoch, phase=\"test\")\n",
    "        self._save_submission(preds_epoch) \n",
    "        \n",
    "        self.test_preds.clear()\n",
    "        self.test_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def _evaluate(self, preds_np, trues_np, phase=\"val\"):\n",
    "        \"\"\"Calculates and logs evaluation metrics.\"\"\"\n",
    "        # This check is important for when _evaluate might be called outside trainer.fit/test context\n",
    "        # or if datamodule is not correctly propagated.\n",
    "        if self.trainer.datamodule is None or not hasattr(self.trainer.datamodule, 'get_lat_weights'):\n",
    "            print(\"Warning: self.trainer.datamodule not fully available in _evaluate. Using fallback for coords/weights.\")\n",
    "            dm_eval = ClimateDataModule(**config[\"data\"]) # Re-init for coords, assumes global config\n",
    "            dm_eval.prepare_data()\n",
    "            dm_eval.setup(stage=phase) # Setup for the correct stage\n",
    "            area_weights = dm_eval.get_lat_weights()\n",
    "            lat, lon = dm_eval.get_coords()\n",
    "            output_vars = dm_eval.hparams.output_vars\n",
    "        else:\n",
    "            area_weights = self.trainer.datamodule.get_lat_weights()\n",
    "            lat, lon = self.trainer.datamodule.get_coords()\n",
    "            output_vars = self.trainer.datamodule.hparams.output_vars\n",
    "\n",
    "\n",
    "        time_coords = np.arange(preds_np.shape[0])\n",
    "        metrics_summary = {}\n",
    "\n",
    "        for i, var_name in enumerate(output_vars):\n",
    "            p_var = preds_np[:, i] \n",
    "            t_var = trues_np[:, i] \n",
    "            \n",
    "            p_xr = xr.DataArray(p_var, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time_coords, \"y\": lat, \"x\": lon})\n",
    "            t_xr = xr.DataArray(t_var, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time_coords, \"y\": lat, \"x\": lon})\n",
    "\n",
    "            rmse = np.sqrt(((p_xr - t_xr) ** 2).weighted(area_weights).mean()).item()\n",
    "            mean_rmse = np.sqrt(((p_xr.mean(\"time\") - t_xr.mean(\"time\")) ** 2).weighted(area_weights).mean()).item()\n",
    "            std_mae = np.abs(p_xr.std(\"time\") - t_xr.std(\"time\")).weighted(area_weights).mean().item()\n",
    "\n",
    "            print(f\"[{phase.upper()}] {var_name}: RMSE={rmse:.4f}, Time-Mean RMSE={mean_rmse:.4f}, Time-Stddev MAE={std_mae:.4f}\")\n",
    "            \n",
    "            metrics_summary[f\"{phase}/{var_name}/rmse\"] = rmse\n",
    "            metrics_summary[f\"{phase}/{var_name}/time_mean_rmse\"] = mean_rmse\n",
    "            metrics_summary[f\"{phase}/{var_name}/time_std_mae\"] = std_mae\n",
    "        \n",
    "        self.log_dict(metrics_summary, logger=True)\n",
    "\n",
    "    def _save_submission(self, predictions_np):\n",
    "        \"\"\"Saves model predictions to a CSV file in Kaggle submission format.\"\"\"\n",
    "        if self.trainer.datamodule is None or not hasattr(self.trainer.datamodule, 'get_coords'):\n",
    "            print(\"Warning: self.trainer.datamodule not fully available in _save_submission. Using fallback.\")\n",
    "            dm_submission = ClimateDataModule(**config[\"data\"])\n",
    "            dm_submission.prepare_data()\n",
    "            dm_submission.setup(stage=\"test\") # Ensure coords are loaded\n",
    "            lat, lon = dm_submission.get_coords()\n",
    "            output_vars = dm_submission.hparams.output_vars\n",
    "        else:\n",
    "            lat, lon = self.trainer.datamodule.get_coords()\n",
    "            output_vars = self.trainer.datamodule.hparams.output_vars\n",
    "            \n",
    "        time_coords_submission = np.arange(predictions_np.shape[0])\n",
    "\n",
    "        rows = []\n",
    "        for t_idx, t_val in enumerate(time_coords_submission):\n",
    "            for var_idx, var_name in enumerate(output_vars):\n",
    "                for y_idx, y_val in enumerate(lat):\n",
    "                    for x_idx, x_val in enumerate(lon):\n",
    "                        row_id = f\"t{t_idx:03d}_{var_name}_{y_val:.2f}_{x_val:.2f}\"\n",
    "                        pred_value = predictions_np[t_idx, var_idx, y_idx, x_idx]\n",
    "                        rows.append({\"ID\": row_id, \"Prediction\": pred_value})\n",
    "\n",
    "        submission_df = pd.DataFrame(rows)\n",
    "        submission_dir = \"submissions\"\n",
    "        os.makedirs(submission_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filepath = os.path.join(submission_dir, f\"kaggle_submission_unet_{timestamp}.csv\")\n",
    "        submission_df.to_csv(filepath, index=False)\n",
    "        print(f\"✅ Submission saved to: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0de72c4-c536-470a-8a26-e4a73a666f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training and Evaluation Script\n",
    "\n",
    "# --- Instantiate DataModule ---\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "# datamodule.prepare_data() # Called by Trainer when .fit() or .test() is called\n",
    "# datamodule.setup()      # Called by Trainer when .fit() or .test() is called\n",
    "\n",
    "# --- Instantiate U-Net Model ---\n",
    "n_inputs = len(config[\"data\"][\"input_vars\"])\n",
    "n_outputs = len(config[\"data\"][\"output_vars\"])\n",
    "\n",
    "unet_config_params = config.get(\"model_unet\", {}) \n",
    "init_features = unet_config_params.get(\"init_features\", 64)\n",
    "bilinear_upsampling = unet_config_params.get(\"bilinear\", True)\n",
    "\n",
    "unet_model = UNet(n_input_channels=n_inputs, \n",
    "                  n_output_channels=n_outputs, \n",
    "                  init_features=init_features,\n",
    "                  bilinear=bilinear_upsampling)\n",
    "\n",
    "# --- Instantiate Lightning Module ---\n",
    "learning_rate = config[\"training\"][\"lr\"]\n",
    "lightning_module = ClimateEmulationModule(unet_model, learning_rate=learning_rate)\n",
    "\n",
    "# --- Setup Trainer ---\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val/loss\", \n",
    "    mode=\"min\",         \n",
    "    filename=\"unet-best-{epoch:02d}-{val/loss:.2f}\", \n",
    "    save_top_k=1,       \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val/loss\",\n",
    "    patience=5, \n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer_params = {**config[\"trainer\"]} \n",
    "trainer_params[\"callbacks\"] = [checkpoint_callback, early_stop_callback]\n",
    "# Optional: Add logger\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"unet_climate_emulation\")\n",
    "# trainer_params[\"logger\"] = logger\n",
    "\n",
    "trainer = pl.Trainer(**trainer_params)\n",
    "\n",
    "# # --- Train the Model ---\n",
    "# print(\"Starting U-Net model training...\")\n",
    "# trainer.fit(lightning_module, datamodule=datamodule)\n",
    "# print(\"Training finished.\")\n",
    "\n",
    "# # --- Test the Model ---\n",
    "# print(\"Starting U-Net model testing using the best checkpoint...\")\n",
    "# # trainer.test will use the checkpoint_callback's best_model_path by default if available\n",
    "# # or you can specify ckpt_path=\"best\"\n",
    "# test_results = trainer.test(lightning_module, datamodule=datamodule, ckpt_path=\"best\") \n",
    "# print(\"Testing finished.\")\n",
    "# print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ea3c62-3eb2-4a17-bacb-d49850478666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Plotting Utilities (Optional)\n",
    "# # Ensure matplotlib, numpy, and xarray are imported (usually in Cell 1)\n",
    "\n",
    "# def plot_comparison(true_xr, pred_xr, title, cmap='viridis', diff_cmap='RdBu_r', metric_val=None, metric_name=\"Metric\"):\n",
    "#     \"\"\"\n",
    "#     Plots a comparison between ground truth, prediction, and their difference.\n",
    "#     Includes calculation and display of a spatial mean metric (e.g., RMSE).\n",
    "#     \"\"\"\n",
    "#     fig, axs = plt.subplots(1, 3, figsize=(18, 5)) \n",
    "#     fig.suptitle(title, fontsize=16) \n",
    "\n",
    "#     common_min = min(true_xr.min().item(), pred_xr.min().item())\n",
    "#     common_max = max(true_xr.max().item(), pred_xr.max().item())\n",
    "\n",
    "#     true_xr.plot(ax=axs[0], cmap=cmap, vmin=common_min, vmax=common_max, add_colorbar=True, cbar_kwargs={'label': true_xr.name or 'Value'})\n",
    "#     axs[0].set_title(f\"Ground Truth\")\n",
    "\n",
    "#     pred_xr.plot(ax=axs[1], cmap=cmap, vmin=common_min, vmax=common_max, add_colorbar=True, cbar_kwargs={'label': pred_xr.name or 'Value'})\n",
    "#     axs[1].set_title(f\"Prediction\")\n",
    "\n",
    "#     diff = pred_xr - true_xr\n",
    "#     abs_max_diff = np.max(np.abs(diff.data)) if diff.size > 0 else 0.1 \n",
    "    \n",
    "#     diff_plot_params = {'cmap': diff_cmap, 'add_colorbar': True, 'cbar_kwargs': {'label': 'Difference'}}\n",
    "#     if abs_max_diff > 0: \n",
    "#         diff_plot_params['vmin'] = -abs_max_diff\n",
    "#         diff_plot_params['vmax'] = abs_max_diff\n",
    "        \n",
    "#     diff.plot(ax=axs[2], **diff_plot_params)\n",
    "    \n",
    "#     title_suffix = \"\"\n",
    "#     if metric_val is not None:\n",
    "#         title_suffix = f\" ({metric_name}: {metric_val:.4f})\"\n",
    "#     axs[2].set_title(f\"Difference (Pred - Truth){title_suffix}\")\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08db76cd-147a-45aa-9b40-bfe686da30e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Cell 10: Visualization Script (Optional)\n",
    "\n",
    "# try:\n",
    "#     val_preds_loaded = np.load(\"val_preds.npy\")\n",
    "#     val_trues_loaded = np.load(\"val_trues.npy\")\n",
    "\n",
    "#     if not hasattr(datamodule, 'lat') or datamodule.lat is None:\n",
    "#         print(\"Datamodule not fully set up for visualization. Setting it up...\")\n",
    "#         # datamodule.prepare_data() # Should have been called by trainer\n",
    "#         datamodule.setup(stage=\"fit\") # Ensure lat, lon, etc. are available\n",
    "\n",
    "#     lat, lon = datamodule.get_coords()\n",
    "#     output_vars = config[\"data\"][\"output_vars\"] \n",
    "#     area_weights_vis = datamodule.get_lat_weights() \n",
    "    \n",
    "#     time_val_coords = np.arange(val_preds_loaded.shape[0])\n",
    "\n",
    "#     print(f\"\\n--- Visualizing Validation Predictions for U-Net ---\")\n",
    "#     for i, var_name in enumerate(output_vars):\n",
    "#         pred_xr = xr.DataArray(val_preds_loaded[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "#                                coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "#         true_xr = xr.DataArray(val_trues_loaded[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "#                                coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "\n",
    "#         pred_mean = pred_xr.mean(\"time\")\n",
    "#         true_mean = true_xr.mean(\"time\")\n",
    "#         mean_rmse_var = np.sqrt(((pred_mean - true_mean) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "#         plot_comparison(true_mean, pred_mean, \n",
    "#                         f\"U-Net: {var_name.upper()} - Validation Time-Mean\",\n",
    "#                         metric_val=mean_rmse_var, metric_name=\"Time-Mean RMSE\")\n",
    "\n",
    "#         pred_std = pred_xr.std(\"time\")\n",
    "#         true_std = true_xr.std(\"time\")\n",
    "#         std_mae_var = np.abs(pred_std - true_std).weighted(area_weights_vis).mean().item()\n",
    "#         plot_comparison(true_std, pred_std, \n",
    "#                         f\"U-Net: {var_name.upper()} - Validation Time-StdDev\", cmap=\"plasma\",\n",
    "#                         metric_val=std_mae_var, metric_name=\"Time-StdDev MAE\")\n",
    "\n",
    "#         if len(time_val_coords) > 0:\n",
    "#             t_idx_random = np.random.randint(0, len(time_val_coords))\n",
    "#             pred_sample = pred_xr.isel(time=t_idx_random)\n",
    "#             true_sample = true_xr.isel(time=t_idx_random)\n",
    "#             sample_rmse_var = np.sqrt(((pred_sample - true_sample) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "#             plot_comparison(true_sample, pred_sample, \n",
    "#                             f\"U-Net: {var_name.upper()} - Validation Sample (Timestep {t_idx_random})\",\n",
    "#                             metric_val=sample_rmse_var, metric_name=\"RMSE\")\n",
    "#         else:\n",
    "#             print(f\"No time steps available in validation predictions for {var_name} to plot a random sample.\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"val_preds.npy or val_trues.npy not found. \"\n",
    "#           \"Ensure that the training and validation loop (trainer.fit) has been run successfully, \"\n",
    "#           \"and the on_validation_epoch_end method in ClimateEmulationModule saved these files.\")\n",
    "# except AttributeError as e:\n",
    "#     print(f\"AttributeError during visualization: {e}. Ensure datamodule is correctly initialized and set up.\")\n",
    "#     print(\"This might happen if 'datamodule' from the training cell is not in scope or wasn't fully set up.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during visualization: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2292f-7977-48aa-912d-f590f2b11109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning Run 1/11 ---\n",
      "Parameters: {'unet_init_features': 32, 'lr': 0.0001, 'optimizer_type': 'AdamW', 'scheduler_type': 'CosineAnnealingLR', 'batch_size': 32, 'max_epochs_ft': 20}\n",
      "Fitting model for run 1 with params: {'unet_init_features': 32, 'lr': 0.0001, 'optimizer_type': 'AdamW', 'scheduler_type': 'CosineAnnealingLR', 'batch_size': 32, 'max_epochs_ft': 20}\n",
      "Creating dataset with 2703 samples...\n",
      "Dataset created. Input shape: torch.Size([2703, 5, 48, 72]), Output shape: torch.Size([2703, 2, 48, 72])\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model     | UNet    | 7.9 M  | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "7.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 M     Total params\n",
      "31.401    Total estimated model params size (MB)\n",
      "96        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Input shape: torch.Size([360, 5, 48, 72]), Output shape: torch.Size([360, 2, 48, 72])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadde7f5ed504e3395b107ae9de4d3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=4.7410, Time-Mean RMSE=3.1244, Time-Stddev MAE=1.2422\n",
      "[VAL] pr: RMSE=2.8756, Time-Mean RMSE=1.1028, Time-Stddev MAE=1.6125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.9995, Time-Mean RMSE=2.2376, Time-Stddev MAE=1.2340\n",
      "[VAL] pr: RMSE=2.6539, Time-Mean RMSE=0.8598, Time-Stddev MAE=1.3079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.6766, Time-Mean RMSE=2.0067, Time-Stddev MAE=1.1619\n",
      "[VAL] pr: RMSE=2.4704, Time-Mean RMSE=0.7394, Time-Stddev MAE=1.1523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.3281, Time-Mean RMSE=2.0420, Time-Stddev MAE=0.9135\n",
      "[VAL] pr: RMSE=2.3970, Time-Mean RMSE=0.7634, Time-Stddev MAE=1.0291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.7262, Time-Mean RMSE=1.6727, Time-Stddev MAE=0.7710\n",
      "[VAL] pr: RMSE=2.1593, Time-Mean RMSE=0.5838, Time-Stddev MAE=0.9227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.7197, Time-Mean RMSE=1.5844, Time-Stddev MAE=0.7572\n",
      "[VAL] pr: RMSE=2.1898, Time-Mean RMSE=0.5497, Time-Stddev MAE=0.8620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.4644, Time-Mean RMSE=1.5370, Time-Stddev MAE=0.6800\n",
      "[VAL] pr: RMSE=2.1146, Time-Mean RMSE=0.6062, Time-Stddev MAE=0.8661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.3524, Time-Mean RMSE=1.4069, Time-Stddev MAE=0.6698\n",
      "[VAL] pr: RMSE=2.0565, Time-Mean RMSE=0.4785, Time-Stddev MAE=0.8530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.3165, Time-Mean RMSE=1.3755, Time-Stddev MAE=0.6473\n",
      "[VAL] pr: RMSE=2.0745, Time-Mean RMSE=0.4969, Time-Stddev MAE=0.8731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.2913, Time-Mean RMSE=1.3530, Time-Stddev MAE=0.6360\n",
      "[VAL] pr: RMSE=2.0696, Time-Mean RMSE=0.5003, Time-Stddev MAE=0.8621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1909, Time-Mean RMSE=1.3161, Time-Stddev MAE=0.5769\n",
      "[VAL] pr: RMSE=2.0449, Time-Mean RMSE=0.4882, Time-Stddev MAE=0.8478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.2121, Time-Mean RMSE=1.3162, Time-Stddev MAE=0.5755\n",
      "[VAL] pr: RMSE=2.0398, Time-Mean RMSE=0.4533, Time-Stddev MAE=0.8327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1717, Time-Mean RMSE=1.3087, Time-Stddev MAE=0.5765\n",
      "[VAL] pr: RMSE=2.0352, Time-Mean RMSE=0.4566, Time-Stddev MAE=0.8347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1604, Time-Mean RMSE=1.2580, Time-Stddev MAE=0.5983\n",
      "[VAL] pr: RMSE=2.0296, Time-Mean RMSE=0.4522, Time-Stddev MAE=0.8286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1066, Time-Mean RMSE=1.2145, Time-Stddev MAE=0.5628\n",
      "[VAL] pr: RMSE=2.0235, Time-Mean RMSE=0.4253, Time-Stddev MAE=0.8355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1035, Time-Mean RMSE=1.2774, Time-Stddev MAE=0.5395\n",
      "[VAL] pr: RMSE=2.0088, Time-Mean RMSE=0.4155, Time-Stddev MAE=0.8129\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: U-Net Fine-Tuning Loop\n",
    "\n",
    "from copy import deepcopy\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, TQDMProgressBar\n",
    "\n",
    "# hyperparameter_sets = [\n",
    "#     {\"unet_init_features\": 32, \"lr\": 1e-4, \"optimizer_type\": \"AdamW\", \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 32, \"max_epochs_ft\": 5},\n",
    "#     {\"unet_init_features\": 64, \"lr\": 1e-4, \"optimizer_type\": \"AdamW\", \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 32, \"max_epochs_ft\": 5},\n",
    "#     {\"unet_init_features\": 32, \"lr\": 5e-4, \"optimizer_type\": \"Adam\",  \"scheduler_type\": None,              \"batch_size\": 64, \"max_epochs_ft\": 5},\n",
    "#     {\"unet_init_features\": 64, \"lr\": 1e-3, \"optimizer_type\": \"AdamW\", \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 64, \"max_epochs_ft\": 8},\n",
    "# ]\n",
    "\n",
    "# best: {\"unet_init_features\": 32, \"lr\": 1e-4, \"optimizer_type\": \"AdamW\", \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 32, \"max_epochs_ft\": 5}\n",
    "\n",
    "hyperparameter_sets = [\n",
    "    # original four (gets 1)\n",
    "    {\"unet_init_features\": 32,  \"lr\": 1e-4, \"optimizer_type\": \"AdamW\",   \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 128, \"lr\": 1e-4, \"optimizer_type\": \"AdamW\",   \"scheduler_type\": \"StepLR\",             \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 32,  \"lr\": 1e-3, \"optimizer_type\": \"Adam\",    \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 64,  \"lr\": 5e-4, \"optimizer_type\": \"Adam\",    \"scheduler_type\": \"StepLR\",             \"batch_size\": 16, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 128, \"lr\": 5e-4, \"optimizer_type\": \"AdamW\",   \"scheduler_type\": None,                \"batch_size\": 64, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 32,  \"lr\": 2e-4, \"optimizer_type\": \"AdamW\",   \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 16, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 128, \"lr\": 1e-3, \"optimizer_type\": \"Adam\",    \"scheduler_type\": None,                \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "\n",
    "    # four new combos with different optimizers\n",
    "    {\"unet_init_features\": 32,  \"lr\": 1e-2, \"optimizer_type\": \"SGD\",     \"scheduler_type\": \"StepLR\",             \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 64,  \"lr\": 1e-2, \"optimizer_type\": \"SGD\",     \"scheduler_type\": None,                \"batch_size\": 64, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 32,  \"lr\": 5e-4, \"optimizer_type\": \"RMSprop\", \"scheduler_type\": \"CosineAnnealingLR\", \"batch_size\": 16, \"max_epochs_ft\": 20},\n",
    "    {\"unet_init_features\": 64,  \"lr\": 1e-3, \"optimizer_type\": \"Adagrad\", \"scheduler_type\": \"StepLR\",             \"batch_size\": 32, \"max_epochs_ft\": 20},\n",
    "]\n",
    "\n",
    "\n",
    "fine_tuning_results = []\n",
    "\n",
    "# Original config for data path and other fixed settings\n",
    "base_config_data_path = config[\"data\"][\"path\"] \n",
    "base_config_trainer_fixed = {k: v for k, v in config[\"trainer\"].items() if k not in [\"max_epochs\", \"callbacks\", \"logger\", \"default_root_dir\"]}\n",
    "\n",
    "\n",
    "for i, params in enumerate(hyperparameter_sets):\n",
    "    print(f\"\\n--- Fine-Tuning Run {i+1}/{len(hyperparameter_sets)} ---\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "\n",
    "    # 1. Create a deep copy of the base config and update it\n",
    "    current_config = deepcopy(config) # Start with the global config\n",
    "    current_config[\"data\"][\"batch_size\"] = params[\"batch_size\"]\n",
    "    current_config[\"model_unet\"][\"init_features\"] = params[\"unet_init_features\"]\n",
    "    # We'll handle lr, optimizer, scheduler directly in LightningModule or Trainer setup\n",
    "    \n",
    "    # 2. Re-instantiate DataModule (important if batch_size changes)\n",
    "    # The datamodule setup (normalization stats) should ideally be based on the full training set once.\n",
    "    # For fine-tuning, if only batch_size changes, re-instantiating is okay.\n",
    "    # If other data aspects change (like SSPs), ensure setup() is appropriate.\n",
    "    datamodule_ft = ClimateDataModule(**current_config[\"data\"])\n",
    "    # datamodule_ft.prepare_data() # Called by Trainer\n",
    "    # datamodule_ft.setup(stage=\"fit\") # Called by Trainer\n",
    "\n",
    "    # 3. Re-instantiate U-Net model\n",
    "    n_inputs_ft = len(current_config[\"data\"][\"input_vars\"])\n",
    "    n_outputs_ft = len(current_config[\"data\"][\"output_vars\"])\n",
    "    \n",
    "    unet_model_ft = UNet(\n",
    "        n_input_channels=n_inputs_ft, \n",
    "        n_output_channels=n_outputs_ft, \n",
    "        init_features=current_config[\"model_unet\"][\"init_features\"],\n",
    "        bilinear=current_config[\"model_unet\"].get(\"bilinear\", True) # Use bilinear from config or default\n",
    "    )\n",
    "\n",
    "    # 4. Re-instantiate LightningModule\n",
    "    # We will override configure_optimizers for this run\n",
    "    lightning_module_ft = ClimateEmulationModule(\n",
    "        unet_model_ft, \n",
    "        learning_rate=params[\"lr\"] # Pass the current learning rate\n",
    "    )\n",
    "\n",
    "    # Override configure_optimizers for the current fine-tuning run\n",
    "    def custom_configure_optimizers(self_lm):\n",
    "        if params[\"optimizer_type\"] == \"AdamW\":\n",
    "            optimizer = optim.AdamW(self_lm.parameters(), lr=self_lm.hparams.learning_rate, weight_decay=0.01)\n",
    "        else: # Default to Adam\n",
    "            optimizer = optim.Adam(self_lm.parameters(), lr=self_lm.hparams.learning_rate)\n",
    "        \n",
    "        if params[\"scheduler_type\"] == \"CosineAnnealingLR\":\n",
    "            # T_max could be params[\"max_epochs_ft\"] or total training steps\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params[\"max_epochs_ft\"], eta_min=1e-6)\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\"}}\n",
    "        return optimizer\n",
    "\n",
    "    # Bind the custom method to this instance of the LightningModule\n",
    "    import types\n",
    "    lightning_module_ft.configure_optimizers = types.MethodType(custom_configure_optimizers, lightning_module_ft)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. Re-instantiate Trainer\n",
    "    # Callbacks for this fine-tuning run\n",
    "    # Using a unique directory for each run's checkpoints/logs can be helpful\n",
    "    run_specific_dir = f\"ft_unet_run_{i+1}\"\n",
    "    \n",
    "    ft_checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=os.path.join(\"lightning_logs\", run_specific_dir, \"checkpoints\"), # Save checkpoints in run-specific dir\n",
    "        filename=\"best-unet-{epoch:02d}-{val/loss:.3f}\",\n",
    "        save_top_k=1,\n",
    "        verbose=False # Less verbose for multiple runs\n",
    "    )\n",
    "    ft_early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val/loss\",\n",
    "        patience=3, # Shorter patience for fine-tuning runs\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "\n",
    "        # Only keep early stopping & progress bar—no checkpoints or tensorboard logs\n",
    "     ft_early_stop_callback = EarlyStopping(\n",
    "         monitor=\"val/loss\", patience=3, mode=\"min\"\n",
    "     )\n",
    "     ft_progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    " \n",
    "     trainer_ft = pl.Trainer(\n",
    "         max_epochs=params[\"max_epochs_ft\"],\n",
    "         callbacks=[ft_early_stop_callback, ft_progress_bar],\n",
    "         logger=False, \n",
    "         enable_checkpointing=False,\n",
    "         **base_config_trainer_fixed\n",
    "     )\n",
    "\n",
    "    ft_progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "    trainer_ft_config = {\n",
    "        **base_config_trainer_fixed, # Use fixed parts of trainer config\n",
    "        \"max_epochs\": params[\"max_epochs_ft\"],\n",
    "        \"callbacks\": [ft_checkpoint_callback, ft_early_stop_callback, ft_progress_bar],\n",
    "        \"logger\": pl.loggers.TensorBoardLogger(\"tb_logs\", name=f\"unet_ft_run_{i+1}\"), # Log each run separately\n",
    "        \"default_root_dir\": os.path.join(\"lightning_logs\", run_specific_dir) # Root dir for this run\n",
    "    }\n",
    "    trainer_ft = pl.Trainer(**trainer_ft_config)\n",
    "\n",
    "    # 6. Run training\n",
    "    print(f\"Fitting model for run {i+1} with params: {params}\")\n",
    "    trainer_ft.fit(lightning_module_ft, datamodule=datamodule_ft)\n",
    "    \n",
    "    # 7. Evaluate on the validation set using the best checkpoint from this run\n",
    "    print(f\"Evaluating model from run {i+1} on validation set...\")\n",
    "    # The test method here is used for validation set evaluation for fine-tuning\n",
    "    # Ensure your ClimateEmulationModule's test_step and on_test_epoch_end\n",
    "    # are suitable for this (e.g., they log \"test/...\" metrics which you'd interpret as \"finetune_val/...\")\n",
    "    # Or, better, use trainer.validate if you only need validation metrics.\n",
    "    # For simplicity, let's assume trainer.test() is okay and we look at its output.\n",
    "    # The checkpoint_callback saves the best model based on \"val/loss\".\n",
    "    \n",
    "    val_results = trainer_ft.validate(lightning_module_ft, datamodule=datamodule_ft, ckpt_path=\"best\")\n",
    "    best_val_loss = val_results[0].get('val/loss', float('inf')) # Get the final validation loss\n",
    "\n",
    "    # You might also want to run .test() on the actual test set if you want to see\n",
    "    # how each hyperparameter set performs on the final test data, but typically\n",
    "    # hyperparameter tuning is done based on validation set performance.\n",
    "    # For now, we'll store validation results.\n",
    "    \n",
    "    current_run_results = {\n",
    "        \"params\": params,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"val_metrics\": val_results[0] # Store all metrics from validation\n",
    "        # \"test_metrics_on_val_split\": test_on_val_results[0] # If you used .test on val_dataloader\n",
    "    }\n",
    "    fine_tuning_results.append(current_run_results)\n",
    "    print(f\"Run {i+1} Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "# 8. Analyze fine-tuning results\n",
    "print(\"\\n--- Fine-Tuning Complete ---\")\n",
    "best_run = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for result in fine_tuning_results:\n",
    "    print(f\"Params: {result['params']}, Best Val Loss: {result['best_val_loss']:.4f}\")\n",
    "    if result['best_val_loss'] < best_loss:\n",
    "        best_loss = result['best_val_loss']\n",
    "        best_run = result\n",
    "\n",
    "if best_run:\n",
    "    print(f\"\\nBest performing set of parameters:\")\n",
    "    print(f\"Params: {best_run['params']}\")\n",
    "    print(f\"Validation Loss: {best_run['best_val_loss']:.4f}\")\n",
    "    print(f\"Full Validation Metrics: {best_run['val_metrics']}\")\n",
    "else:\n",
    "    print(\"No fine-tuning runs completed or no results to analyze.\")\n",
    "\n",
    "# After identifying the best hyperparameters, you would typically retrain your model\n",
    "# on the full training data (including the part of ssp370 used for validation here)\n",
    "# for a larger number of epochs, and then evaluate on the final held-out test set (ssp245).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08fda5-60ab-4820-b87a-b79c7e8b7973",
   "metadata": {},
   "source": [
    "best_params_unet: {'unet_init_features': 64,\n",
    " 'lr': 0.0001,\n",
    " 'optimizer_type': 'AdamW',\n",
    " 'scheduler_type': 'CosineAnnealingLR',\n",
    " 'batch_size': 32,\n",
    " 'max_epochs_ft': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ea2691-663a-44a3-b4ba-0fb5976763dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unet_init_features': 64,\n",
       " 'lr': 0.0001,\n",
       " 'optimizer_type': 'AdamW',\n",
       " 'scheduler_type': 'CosineAnnealingLR',\n",
       " 'batch_size': 32,\n",
       " 'max_epochs_ft': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7987d283-8ad2-468c-a30e-5f56185aba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting U-Net Final Training with Best Hyperparameters ---\n",
      "Using best U-Net parameters: {'unet_init_features': 64, 'lr': 0.0001, 'optimizer_type': 'AdamW', 'scheduler_type': 'CosineAnnealingLR', 'batch_size': 32, 'max_epochs_ft': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final U-Net model with best params: {'unet_init_features': 64, 'lr': 0.0001, 'optimizer_type': 'AdamW', 'scheduler_type': 'CosineAnnealingLR', 'batch_size': 32, 'max_epochs_ft': 5}\n",
      "Creating dataset with 2703 samples...\n",
      "Dataset created. Input shape: torch.Size([2703, 5, 48, 72]), Output shape: torch.Size([2703, 2, 48, 72])\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model     | UNet    | 31.4 M | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "31.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.4 M    Total params\n",
      "125.544   Total estimated model params size (MB)\n",
      "96        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Input shape: torch.Size([360, 5, 48, 72]), Output shape: torch.Size([360, 2, 48, 72])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ed18c6623e40c89dd91d921ccc741c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 0.354\n",
      "Epoch 0, global step 85: 'val/loss' reached 0.35412 (best 0.35412), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=00-val/loss=0.354.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=4.6756, Time-Mean RMSE=2.9997, Time-Stddev MAE=1.4171\n",
      "[VAL] pr: RMSE=2.8718, Time-Mean RMSE=1.1280, Time-Stddev MAE=1.6364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.104 >= min_delta = 0.0. New best score: 0.250\n",
      "Epoch 1, global step 170: 'val/loss' reached 0.25040 (best 0.25040), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=01-val/loss=0.250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.3871, Time-Mean RMSE=2.0922, Time-Stddev MAE=0.8684\n",
      "[VAL] pr: RMSE=2.4418, Time-Mean RMSE=0.7451, Time-Stddev MAE=1.1813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.032 >= min_delta = 0.0. New best score: 0.218\n",
      "Epoch 2, global step 255: 'val/loss' reached 0.21838 (best 0.21838), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=02-val/loss=0.218.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.1998, Time-Mean RMSE=2.1231, Time-Stddev MAE=0.8184\n",
      "[VAL] pr: RMSE=2.2566, Time-Mean RMSE=0.6638, Time-Stddev MAE=0.9735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.039 >= min_delta = 0.0. New best score: 0.179\n",
      "Epoch 3, global step 340: 'val/loss' reached 0.17946 (best 0.17946), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=03-val/loss=0.179.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.3045, Time-Mean RMSE=1.5245, Time-Stddev MAE=0.5885\n",
      "[VAL] pr: RMSE=2.0808, Time-Mean RMSE=0.5461, Time-Stddev MAE=0.8473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.004 >= min_delta = 0.0. New best score: 0.175\n",
      "Epoch 4, global step 425: 'val/loss' reached 0.17514 (best 0.17514), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=04-val/loss=0.175.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.2324, Time-Mean RMSE=1.4714, Time-Stddev MAE=0.5525\n",
      "[VAL] pr: RMSE=2.0589, Time-Mean RMSE=0.5192, Time-Stddev MAE=0.8460\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 0.0. New best score: 0.175\n",
      "Epoch 5, global step 510: 'val/loss' reached 0.17507 (best 0.17507), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=05-val/loss=0.175.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1329, Time-Mean RMSE=1.3450, Time-Stddev MAE=0.5243\n",
      "[VAL] pr: RMSE=2.0633, Time-Mean RMSE=0.5476, Time-Stddev MAE=0.8276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.003 >= min_delta = 0.0. New best score: 0.172\n",
      "Epoch 6, global step 595: 'val/loss' reached 0.17226 (best 0.17226), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=06-val/loss=0.172.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.1397, Time-Mean RMSE=1.3708, Time-Stddev MAE=0.5376\n",
      "[VAL] pr: RMSE=2.0468, Time-Mean RMSE=0.4955, Time-Stddev MAE=0.8550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.005 >= min_delta = 0.0. New best score: 0.167\n",
      "Epoch 7, global step 680: 'val/loss' reached 0.16720 (best 0.16720), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=07-val/loss=0.167.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.9983, Time-Mean RMSE=1.2085, Time-Stddev MAE=0.4942\n",
      "[VAL] pr: RMSE=2.0186, Time-Mean RMSE=0.4482, Time-Stddev MAE=0.8197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.0. New best score: 0.166\n",
      "Epoch 8, global step 765: 'val/loss' reached 0.16615 (best 0.16615), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=08-val/loss=0.166.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.9510, Time-Mean RMSE=1.1619, Time-Stddev MAE=0.4505\n",
      "[VAL] pr: RMSE=2.0146, Time-Mean RMSE=0.4161, Time-Stddev MAE=0.8049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 0.0. New best score: 0.164\n",
      "Epoch 9, global step 850: 'val/loss' reached 0.16437 (best 0.16437), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=09-val/loss=0.164.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.9372, Time-Mean RMSE=1.1764, Time-Stddev MAE=0.4260\n",
      "[VAL] pr: RMSE=2.0040, Time-Mean RMSE=0.4110, Time-Stddev MAE=0.8258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 935: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.8836, Time-Mean RMSE=1.0863, Time-Stddev MAE=0.4396\n",
      "[VAL] pr: RMSE=2.0131, Time-Mean RMSE=0.4628, Time-Stddev MAE=0.8137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 0.0. New best score: 0.163\n",
      "Epoch 11, global step 1020: 'val/loss' reached 0.16282 (best 0.16282), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=11-val/loss=0.163.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.8246, Time-Mean RMSE=1.0323, Time-Stddev MAE=0.4154\n",
      "[VAL] pr: RMSE=1.9970, Time-Mean RMSE=0.4027, Time-Stddev MAE=0.7839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.0. New best score: 0.162\n",
      "Epoch 12, global step 1105: 'val/loss' reached 0.16156 (best 0.16156), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=12-val/loss=0.162.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.8301, Time-Mean RMSE=1.0540, Time-Stddev MAE=0.4153\n",
      "[VAL] pr: RMSE=1.9882, Time-Mean RMSE=0.3885, Time-Stddev MAE=0.7937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 0.0. New best score: 0.161\n",
      "Epoch 13, global step 1190: 'val/loss' reached 0.16123 (best 0.16123), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=13-val/loss=0.161.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7929, Time-Mean RMSE=1.0121, Time-Stddev MAE=0.4022\n",
      "[VAL] pr: RMSE=1.9881, Time-Mean RMSE=0.3910, Time-Stddev MAE=0.7936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 0.0. New best score: 0.161\n",
      "Epoch 14, global step 1275: 'val/loss' reached 0.16096 (best 0.16096), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=14-val/loss=0.161.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7799, Time-Mean RMSE=0.9767, Time-Stddev MAE=0.4033\n",
      "[VAL] pr: RMSE=1.9862, Time-Mean RMSE=0.3856, Time-Stddev MAE=0.8032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78ba2feb42c498a97ff7c5319cea61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.0. New best score: 0.160\n",
      "Epoch 15, global step 1360: 'val/loss' reached 0.16038 (best 0.16038), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=15-val/loss=0.160.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7839, Time-Mean RMSE=0.9797, Time-Stddev MAE=0.4022\n",
      "[VAL] pr: RMSE=1.9827, Time-Mean RMSE=0.3641, Time-Stddev MAE=0.8098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcb920c8e7049d9ac6fcb6f58b254ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.001 >= min_delta = 0.0. New best score: 0.159\n",
      "Epoch 16, global step 1445: 'val/loss' reached 0.15911 (best 0.15911), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=16-val/loss=0.159.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7470, Time-Mean RMSE=0.9589, Time-Stddev MAE=0.3870\n",
      "[VAL] pr: RMSE=1.9752, Time-Mean RMSE=0.3516, Time-Stddev MAE=0.7673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8d7133cfe14b1fa58c17cc5ce1dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 1530: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7566, Time-Mean RMSE=0.9641, Time-Stddev MAE=0.3989\n",
      "[VAL] pr: RMSE=1.9796, Time-Mean RMSE=0.3682, Time-Stddev MAE=0.7846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b04b173fb54959be1ba9ca9575d498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 0.0. New best score: 0.159\n",
      "Epoch 18, global step 1615: 'val/loss' reached 0.15865 (best 0.15865), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=18-val/loss=0.159.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7433, Time-Mean RMSE=0.9430, Time-Stddev MAE=0.3912\n",
      "[VAL] pr: RMSE=1.9724, Time-Mean RMSE=0.3459, Time-Stddev MAE=0.7824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a8cabbd9034468b600442bc0151b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 0.0. New best score: 0.158\n",
      "Epoch 19, global step 1700: 'val/loss' reached 0.15841 (best 0.15841), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=19-val/loss=0.158.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=1.7502, Time-Mean RMSE=0.9523, Time-Stddev MAE=0.3925\n",
      "[VAL] pr: RMSE=1.9702, Time-Mean RMSE=0.3376, Time-Stddev MAE=0.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing final U-Net model on test set (ssp245)...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=19-val/loss=0.158.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Input shape: torch.Size([360, 5, 48, 72]), Output shape: torch.Size([360, 2, 48, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/final_unet_run/checkpoints/final-best-unet-epoch=19-val/loss=0.158.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87dc35e13ea41c593db71554906852b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] tas: RMSE=290.5800, Time-Mean RMSE=290.5367, Time-Stddev MAE=3.7093\n",
      "[TEST] pr: RMSE=4.3301, Time-Mean RMSE=3.8674, Time-Stddev MAE=1.3886\n",
      "✅ Submission saved to: submissions/kaggle_submission_unet_20250522_193549.csv\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test/pr/rmse           4.330072402954102\n",
      " test/pr/time_mean_rmse     3.8674283027648926\n",
      "  test/pr/time_std_mae      1.3885520696640015\n",
      "      test/tas/rmse          290.5799865722656\n",
      " test/tas/time_mean_rmse     290.5366516113281\n",
      "  test/tas/time_std_mae     3.7092957496643066\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Final U-Net Test Results (on ssp245): [{'test/tas/rmse': 290.5799865722656, 'test/tas/time_mean_rmse': 290.5366516113281, 'test/tas/time_std_mae': 3.7092957496643066, 'test/pr/rmse': 4.330072402954102, 'test/pr/time_mean_rmse': 3.8674283027648926, 'test/pr/time_std_mae': 1.3885520696640015}]\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: U-Net - Final Training & Testing with Best Hyperparameters\n",
    "\n",
    "print(\"\\n--- Starting U-Net Final Training with Best Hyperparameters ---\")\n",
    "\n",
    "# Ensure 'best_run' exists from the U-Net fine-tuning loop\n",
    "if 'best_run' not in globals() or best_run is None:\n",
    "    print(\"Error: 'best_run' not found. Please run the U-Net fine-tuning loop first.\")\n",
    "else:\n",
    "    print(f\"Using best U-Net parameters: {best_run['params']}\")\n",
    "\n",
    "    # 1. Create a deep copy of the base config and update with best params\n",
    "    final_config_unet = deepcopy(config)\n",
    "    best_params_unet = best_run['params']\n",
    "\n",
    "    final_config_unet[\"data\"][\"batch_size\"] = best_params_unet[\"batch_size\"]\n",
    "    final_config_unet[\"model_unet\"][\"init_features\"] = best_params_unet[\"unet_init_features\"]\n",
    "    # Assuming bilinear is in the main config[\"model_unet\"] or you can add it to best_params_unet if tuned\n",
    "    \n",
    "    # Use a potentially larger number of epochs for final training\n",
    "    final_max_epochs_unet = config[\"trainer\"].get(\"max_epochs\", 20) # Default to original or set a new value\n",
    "\n",
    "    # 2. Re-instantiate DataModule\n",
    "    datamodule_final_unet = ClimateDataModule(**final_config_unet[\"data\"])\n",
    "\n",
    "    # 3. Re-instantiate U-Net model\n",
    "    n_inputs_final_unet = len(final_config_unet[\"data\"][\"input_vars\"])\n",
    "    n_outputs_final_unet = len(final_config_unet[\"data\"][\"output_vars\"])\n",
    "    \n",
    "    unet_model_final = UNet(\n",
    "        n_input_channels=n_inputs_final_unet, \n",
    "        n_output_channels=n_outputs_final_unet, \n",
    "        init_features=final_config_unet[\"model_unet\"][\"init_features\"],\n",
    "        bilinear=final_config_unet[\"model_unet\"].get(\"bilinear\", True)\n",
    "    )\n",
    "\n",
    "    # 4. Re-instantiate LightningModule\n",
    "    lightning_module_final_unet = ClimateEmulationModule(\n",
    "        unet_model_final, \n",
    "        learning_rate=best_params_unet[\"lr\"]\n",
    "    )\n",
    "\n",
    "    # Override configure_optimizers for the final run\n",
    "    def final_configure_optimizers_unet(self_lm):\n",
    "        if best_params_unet[\"optimizer_type\"] == \"AdamW\":\n",
    "            optimizer = optim.AdamW(self_lm.parameters(), lr=self_lm.hparams.learning_rate, weight_decay=0.01)\n",
    "        else: \n",
    "            optimizer = optim.Adam(self_lm.parameters(), lr=self_lm.hparams.learning_rate)\n",
    "        \n",
    "        if best_params_unet[\"scheduler_type\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=final_max_epochs_unet, eta_min=1e-6)\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\"}}\n",
    "        return optimizer\n",
    "\n",
    "    import types\n",
    "    lightning_module_final_unet.configure_optimizers = types.MethodType(final_configure_optimizers_unet, lightning_module_final_unet)\n",
    "\n",
    "    # 5. Re-instantiate Trainer for final run\n",
    "    final_run_dir_unet = \"final_unet_run\"\n",
    "    \n",
    "    final_checkpoint_callback_unet = ModelCheckpoint(\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=os.path.join(\"lightning_logs\", final_run_dir_unet, \"checkpoints\"),\n",
    "        filename=\"final-best-unet-{epoch:02d}-{val/loss:.3f}\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    )\n",
    "    # Longer patience for final training\n",
    "    final_early_stop_callback_unet = EarlyStopping(monitor=\"val/loss\", patience=10, verbose=True, mode=\"min\") \n",
    "    final_progress_bar_unet = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "    base_config_trainer_fixed_final_unet = {k: v for k, v in config[\"trainer\"].items() if k not in [\"max_epochs\", \"callbacks\", \"logger\", \"default_root_dir\"]}\n",
    "    trainer_final_config_unet = {\n",
    "        **base_config_trainer_fixed_final_unet,\n",
    "        \"max_epochs\": final_max_epochs_unet,\n",
    "        \"callbacks\": [final_checkpoint_callback_unet, final_early_stop_callback_unet, final_progress_bar_unet],\n",
    "        \"logger\": pl.loggers.TensorBoardLogger(\"tb_logs\", name=final_run_dir_unet),\n",
    "        \"default_root_dir\": os.path.join(\"lightning_logs\", final_run_dir_unet)\n",
    "    }\n",
    "    trainer_final_unet = pl.Trainer(**trainer_final_config_unet)\n",
    "\n",
    "    # 6. Run final training\n",
    "    print(f\"Fitting final U-Net model with best params: {best_params_unet}\")\n",
    "    trainer_final_unet.fit(lightning_module_final_unet, datamodule=datamodule_final_unet)\n",
    "    \n",
    "    # 7. Test on the actual test set (ssp245) using the best checkpoint\n",
    "    print(f\"Testing final U-Net model on test set (ssp245)...\")\n",
    "    # The lightning_module_final_unet's on_test_epoch_end will save the submission file\n",
    "    final_test_results_unet = trainer_final_unet.test(lightning_module_final_unet, datamodule=datamodule_final_unet, ckpt_path=\"best\")\n",
    "    print(\"Final U-Net Test Results (on ssp245):\", final_test_results_unet)\n",
    "\n",
    "    # Note: val_preds.npy and val_trues.npy will be overwritten by this run's validation phase.\n",
    "    # These will be used by the subsequent visualization cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5033256-ba88-4200-a144-2de9c96b63bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visualizing Validation Predictions for Final U-Net ---\n",
      "NameError during U-Net final visualization: name 'plot_comparison' is not defined. Ensure 'best_run_unet' was defined and the final training cell (Cell 18) ran successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: U-Net - Final Visualization\n",
    "# This cell loads the validation predictions saved during the FINAL U-Net training run and plots them.\n",
    "\n",
    "print(f\"\\n--- Visualizing Validation Predictions for Final U-Net ---\")\n",
    "try:\n",
    "    # Load the saved predictions and true values from the final U-Net run\n",
    "    val_preds_loaded_final_unet = np.load(\"val_preds.npy\") \n",
    "    val_trues_loaded_final_unet = np.load(\"val_trues.npy\")\n",
    "\n",
    "    # Ensure datamodule_final_unet is available from the training cell (Cell 18)\n",
    "    if 'datamodule_final_unet' not in globals() or not hasattr(datamodule_final_unet, 'lat') or datamodule_final_unet.lat is None:\n",
    "        print(\"datamodule_final_unet not fully set up for visualization. Setting it up...\")\n",
    "        datamodule_final_unet_viz = ClimateDataModule(**config[\"data\"]) # Use global config for safety\n",
    "        datamodule_final_unet_viz.setup(stage=\"fit\") \n",
    "        lat, lon = datamodule_final_unet_viz.get_coords()\n",
    "        output_vars = config[\"data\"][\"output_vars\"] \n",
    "        area_weights_vis = datamodule_final_unet_viz.get_lat_weights()\n",
    "    else:\n",
    "        lat, lon = datamodule_final_unet.get_coords()\n",
    "        output_vars = config[\"data\"][\"output_vars\"] \n",
    "        area_weights_vis = datamodule_final_unet.get_lat_weights() \n",
    "    \n",
    "    time_val_coords = np.arange(val_preds_loaded_final_unet.shape[0])\n",
    "\n",
    "    for i, var_name in enumerate(output_vars):\n",
    "        pred_xr_final_unet = xr.DataArray(val_preds_loaded_final_unet[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "                                   coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "        true_xr_final_unet = xr.DataArray(val_trues_loaded_final_unet[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "                                   coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "\n",
    "        pred_mean_final_unet = pred_xr_final_unet.mean(\"time\")\n",
    "        true_mean_final_unet = true_xr_final_unet.mean(\"time\")\n",
    "        mean_rmse_var_final_unet = np.sqrt(((pred_mean_final_unet - true_mean_final_unet) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "        plot_comparison(true_mean_final_unet, pred_mean_final_unet, \n",
    "                        f\"Final U-Net: {var_name.upper()} - Validation Time-Mean\",\n",
    "                        metric_val=mean_rmse_var_final_unet, metric_name=\"Time-Mean RMSE\")\n",
    "\n",
    "        pred_std_final_unet = pred_xr_final_unet.std(\"time\")\n",
    "        true_std_final_unet = true_xr_final_unet.std(\"time\")\n",
    "        std_mae_var_final_unet = np.abs(pred_std_final_unet - true_std_final_unet).weighted(area_weights_vis).mean().item()\n",
    "        plot_comparison(true_std_final_unet, pred_std_final_unet, \n",
    "                        f\"Final U-Net: {var_name.upper()} - Validation Time-StdDev\", cmap=\"plasma\",\n",
    "                        metric_val=std_mae_var_final_unet, metric_name=\"Time-StdDev MAE\")\n",
    "\n",
    "        if len(time_val_coords) > 0:\n",
    "            t_idx_random_final_unet = np.random.randint(0, len(time_val_coords))\n",
    "            pred_sample_final_unet = pred_xr_final_unet.isel(time=t_idx_random_final_unet)\n",
    "            true_sample_final_unet = true_xr_final_unet.isel(time=t_idx_random_final_unet)\n",
    "            sample_rmse_var_final_unet = np.sqrt(((pred_sample_final_unet - true_sample_final_unet) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "            plot_comparison(true_sample_final_unet, pred_sample_final_unet, \n",
    "                            f\"Final U-Net: {var_name.upper()} - Validation Sample (Timestep {t_idx_random_final_unet})\",\n",
    "                            metric_val=sample_rmse_var_final_unet, metric_name=\"RMSE\")\n",
    "        else:\n",
    "            print(f\"No time steps available in validation predictions for {var_name} to plot a random sample.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"NameError during U-Net final visualization: {e}. Ensure 'best_run_unet' was defined and the final training cell (Cell 18) ran successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"val_preds.npy or val_trues.npy not found for final U-Net run. \"\n",
    "          \"Ensure that the final U-Net training and validation (Cell 18) has run successfully.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError during final U-Net visualization: {e}. Ensure datamodule_final_unet is correctly initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during final U-Net visualization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d7525-d12e-4a72-bc9a-b54e17d4c9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
